{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f484906e-f6d2-4ed5-a9e3-75f371998912",
   "metadata": {},
   "source": [
    "# Run a NN with x and $\\xi$ as the input to the NN model and second stage objective as the target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429642cc-aa1a-4c00-a906-cc7e7bd1fb90",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756a420e-e543-405e-b33f-7c84d7744e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0091ece-a4d3-499f-b013-372a75aacdad",
   "metadata": {},
   "source": [
    "## Load the data of first stage decisions, uncertainities, instances and second stage objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122bf3d7-2361-48d5-b880-14e222c8a74b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ztank/scratch/user/u.rd143338/ss_from_nn/Neural_second_stage'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b91b9984-38fe-44b2-9218-5da13236a269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Directory of data for 10 items\n",
    "\n",
    "direc = pwd()\n",
    "filename = \"instance_1_250_items_30_num_of_first_stage_11_scenarios_100_total_obj\"\n",
    "\n",
    "# path contains the location to the csv files\n",
    "path=os.path.join(direc, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65730a96-d1c7-4c89-a505-f2ea92051734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Directory for saving files\n",
    "\n",
    "direc = pwd()\n",
    "filename = \"instance 30 scenario 50\"\n",
    "\n",
    "# path contains the location to the csv files\n",
    "path_save=os.path.join(direc, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8a4e25-36f3-4ddf-b8d3-4a55db26166c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ztank/scratch/user/u.rd143338/ss_from_nn/Neural_second_stage/instance 30 scenario 50'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2d23136-2e2d-4185-8dbb-a92f1657356e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_weights_biases_to_json(model, folder, filename):\n",
    "    # Extract weights and biases from the model\n",
    "    weights_biases = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        weights_biases[name] = param.data.tolist()\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # Save weights and biases to a JSON file\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(weights_biases, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4624dd-3435-4092-b58e-d8bdb3b17ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_csv_data(folder_path):\n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "    \n",
    "    # Initialize an empty DataFrame to store combined data\n",
    "    dfs = []\n",
    "    \n",
    "    # Loop through each CSV file\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Append the DataFrame to the combined DataFrame\n",
    "        dfs.append(df)\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "## Combined data from csv files\n",
    "\n",
    "folder_path = path\n",
    "combined_data = combine_csv_data(folder_path)\n",
    "combined_test_data = combined_data[combined_data[\"seed\"]>200].reset_index(drop=True)\n",
    "combined_data = combined_data[combined_data[\"seed\"]<=200].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e7d92-2f30-4be2-ba1f-6505c0e122d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9833d8c7-be79-4cfb-98b5-68d3f26aa48f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110000, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6e97e-aa58-434c-9d97-06be4c8f6095",
   "metadata": {},
   "source": [
    "## Shuffle the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95942b47-44a7-4a27-84e4-32e4646a3c52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>x</th>\n",
       "      <th>r</th>\n",
       "      <th>second_stage_obj</th>\n",
       "      <th>First stage value</th>\n",
       "      <th>p_bar</th>\n",
       "      <th>Reduced Capacity</th>\n",
       "      <th>t</th>\n",
       "      <th>p_hat</th>\n",
       "      <th>y</th>\n",
       "      <th>original_capacity</th>\n",
       "      <th>w</th>\n",
       "      <th>uncern</th>\n",
       "      <th>gamma</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[396.51219577782365, 1297.8095999554187, 613.7...</td>\n",
       "      <td>[-0.0, 1.0, 1.0, -0.0, -0.0, 1.0, 1.0, 1.0, -0...</td>\n",
       "      <td>[0.0, 1.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0...</td>\n",
       "      <td>-20095.336990</td>\n",
       "      <td>-16027.892714</td>\n",
       "      <td>[355.7005202119573, 993.3520692471163, 448.251...</td>\n",
       "      <td>11101.129865</td>\n",
       "      <td>[658.9507167826202, 17.97450708363044, 402.464...</td>\n",
       "      <td>[308.1957699511097, 745.5244443487112, 100.020...</td>\n",
       "      <td>[0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, ...</td>\n",
       "      <td>11685.399858</td>\n",
       "      <td>[853.611307630359, 300.00726164821776, 723.006...</td>\n",
       "      <td>[0.973, 0.2367, 0.007, 0.1573, 0.3078, 0.1532,...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[623.6289195525793, 1081.5923852784172, 612.38...</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>-16263.838092</td>\n",
       "      <td>-13166.418746</td>\n",
       "      <td>[451.24504903570255, 828.5511121456775, 500.34...</td>\n",
       "      <td>6055.474844</td>\n",
       "      <td>[520.3593549581708, 39.10659894389094, 286.759...</td>\n",
       "      <td>[292.0484114336391, 216.81785799918933, 151.00...</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, ...</td>\n",
       "      <td>6055.474844</td>\n",
       "      <td>[613.3055991877771, 440.2250701920572, 291.024...</td>\n",
       "      <td>[0.2969, 0.1141, 0.1906, 0.0383, 0.0057, 0.014...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1253.4265962772395, 556.4247037403201, 1199.2...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.0, -0.0, 1.0, 0.0,...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-13137.218650</td>\n",
       "      <td>-10012.129735</td>\n",
       "      <td>[981.6582930980816, 383.32510821236207, 863.99...</td>\n",
       "      <td>4453.682894</td>\n",
       "      <td>[26.692930038231935, 194.57743276540472, 221.0...</td>\n",
       "      <td>[614.1671076219156, 120.6110418808221, 588.167...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "      <td>4948.536549</td>\n",
       "      <td>[232.3117396742037, 303.24539937694857, 272.11...</td>\n",
       "      <td>[0.1155, 0.0884, 0.0358, 0.0336, 0.088, 0.0264...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[323.08191164133547, 1203.2056738115616, 1133....</td>\n",
       "      <td>[-0.0, 1.0, 1.0, 1.0, -0.0, 1.0, 1.0, -0.0, 1....</td>\n",
       "      <td>[0.0, 1.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 1.0...</td>\n",
       "      <td>-18684.906823</td>\n",
       "      <td>-14448.080570</td>\n",
       "      <td>[272.2318350006311, 861.5189571203193, 760.206...</td>\n",
       "      <td>10129.733790</td>\n",
       "      <td>[735.364755016534, 93.76124096413382, 530.2067...</td>\n",
       "      <td>[141.27353893889182, 424.45636372006715, 409.1...</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, ...</td>\n",
       "      <td>11576.838617</td>\n",
       "      <td>[944.638879203141, 667.9007690187085, 875.5204...</td>\n",
       "      <td>[0.1799, 0.0902, 0.0378, 0.0171, 0.0235, 0.051...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[702.4213615041667, 854.7385241285273, 1297.18...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.0, 1.0,...</td>\n",
       "      <td>[-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0,...</td>\n",
       "      <td>-18641.801475</td>\n",
       "      <td>-14374.188370</td>\n",
       "      <td>[619.4876425897213, 630.9511011185651, 978.567...</td>\n",
       "      <td>8924.598138</td>\n",
       "      <td>[370.4243386903249, 20.728789708443763, 229.63...</td>\n",
       "      <td>[454.5042398553577, 524.528056085521, 501.9825...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, ...</td>\n",
       "      <td>11515.610501</td>\n",
       "      <td>[654.065418918689, 153.50070130538387, 791.126...</td>\n",
       "      <td>[0.0009, 0.227, 0.0927, 0.1838, 0.0729, 0.09, ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   f  \\\n",
       "0  [396.51219577782365, 1297.8095999554187, 613.7...   \n",
       "1  [623.6289195525793, 1081.5923852784172, 612.38...   \n",
       "2  [1253.4265962772395, 556.4247037403201, 1199.2...   \n",
       "3  [323.08191164133547, 1203.2056738115616, 1133....   \n",
       "4  [702.4213615041667, 854.7385241285273, 1297.18...   \n",
       "\n",
       "                                                   x  \\\n",
       "0  [-0.0, 1.0, 1.0, -0.0, -0.0, 1.0, 1.0, 1.0, -0...   \n",
       "1  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, ...   \n",
       "2  [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, -0.0, 1.0, 0.0,...   \n",
       "3  [-0.0, 1.0, 1.0, 1.0, -0.0, 1.0, 1.0, -0.0, 1....   \n",
       "4  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.0, 1.0,...   \n",
       "\n",
       "                                                   r  second_stage_obj  \\\n",
       "0  [0.0, 1.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0...     -20095.336990   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...     -16263.838092   \n",
       "2  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...     -13137.218650   \n",
       "3  [0.0, 1.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 1.0...     -18684.906823   \n",
       "4  [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0,...     -18641.801475   \n",
       "\n",
       "   First stage value                                              p_bar  \\\n",
       "0      -16027.892714  [355.7005202119573, 993.3520692471163, 448.251...   \n",
       "1      -13166.418746  [451.24504903570255, 828.5511121456775, 500.34...   \n",
       "2      -10012.129735  [981.6582930980816, 383.32510821236207, 863.99...   \n",
       "3      -14448.080570  [272.2318350006311, 861.5189571203193, 760.206...   \n",
       "4      -14374.188370  [619.4876425897213, 630.9511011185651, 978.567...   \n",
       "\n",
       "   Reduced Capacity                                                  t  \\\n",
       "0      11101.129865  [658.9507167826202, 17.97450708363044, 402.464...   \n",
       "1       6055.474844  [520.3593549581708, 39.10659894389094, 286.759...   \n",
       "2       4453.682894  [26.692930038231935, 194.57743276540472, 221.0...   \n",
       "3      10129.733790  [735.364755016534, 93.76124096413382, 530.2067...   \n",
       "4       8924.598138  [370.4243386903249, 20.728789708443763, 229.63...   \n",
       "\n",
       "                                               p_hat  \\\n",
       "0  [308.1957699511097, 745.5244443487112, 100.020...   \n",
       "1  [292.0484114336391, 216.81785799918933, 151.00...   \n",
       "2  [614.1671076219156, 120.6110418808221, 588.167...   \n",
       "3  [141.27353893889182, 424.45636372006715, 409.1...   \n",
       "4  [454.5042398553577, 524.528056085521, 501.9825...   \n",
       "\n",
       "                                                   y  original_capacity  \\\n",
       "0  [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, ...       11685.399858   \n",
       "1  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, ...        6055.474844   \n",
       "2  [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...        4948.536549   \n",
       "3  [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, ...       11576.838617   \n",
       "4  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, ...       11515.610501   \n",
       "\n",
       "                                                   w  \\\n",
       "0  [853.611307630359, 300.00726164821776, 723.006...   \n",
       "1  [613.3055991877771, 440.2250701920572, 291.024...   \n",
       "2  [232.3117396742037, 303.24539937694857, 272.11...   \n",
       "3  [944.638879203141, 667.9007690187085, 875.5204...   \n",
       "4  [654.065418918689, 153.50070130538387, 791.126...   \n",
       "\n",
       "                                              uncern  gamma  seed  \n",
       "0  [0.973, 0.2367, 0.007, 0.1573, 0.3078, 0.1532,...    6.0    64  \n",
       "1  [0.2969, 0.1141, 0.1906, 0.0383, 0.0057, 0.014...    6.0    81  \n",
       "2  [0.1155, 0.0884, 0.0358, 0.0336, 0.088, 0.0264...    6.0   122  \n",
       "3  [0.1799, 0.0902, 0.0378, 0.0171, 0.0235, 0.051...    4.5    57  \n",
       "4  [0.0009, 0.227, 0.0927, 0.1838, 0.0729, 0.09, ...    6.0   151  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data=combined_data.sample(frac=1, random_state=random.seed(1)).reset_index(drop=True)\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15156b97-a7e3-4a36-ac91-4439ba8e0148",
   "metadata": {},
   "source": [
    "## The element list of the dataframe is split and new dataframe for instances, X and uncertainities are formed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d61c9ef3-5de1-4686-94eb-d6241fbb9a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_list_column(df, col_name):\n",
    "    col=df[col_name].apply(literal_eval).apply(pd.Series)\n",
    "    new_columns = [f'{col_name}_{i}' for i in range(1,len(col.columns)+1)]\n",
    "    col.columns=new_columns\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b9e58ed-206f-43a7-b9bf-bcfe7b3c9a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_parameter = ['f', 'p_bar', 't', 'p_hat', 'original_capacity', 'w', 'gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcf14ad8-00c3-4af4-9ad2-9860817526f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_df=pd.DataFrame()\n",
    "for i in instance_parameter:\n",
    "    if i!='original_capacity' and i!='gamma':\n",
    "        instance_df=pd.concat([instance_df, split_list_column(combined_data,i)], axis=1)\n",
    "    else:\n",
    "        instance_df=pd.concat([instance_df, combined_data[i]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c303ab55-7079-4da6-bd40-80da07ba9e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110000, 152)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4630b5a-1b2c-46d8-b01c-68d06ac20b27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_bar = instance_df.loc[: , instance_df.columns.str.startswith('p_bar')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff648f00-9db9-4f9d-b889-f1f744f6727a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_bar_1</th>\n",
       "      <th>p_bar_2</th>\n",
       "      <th>p_bar_3</th>\n",
       "      <th>p_bar_4</th>\n",
       "      <th>p_bar_5</th>\n",
       "      <th>p_bar_6</th>\n",
       "      <th>p_bar_7</th>\n",
       "      <th>p_bar_8</th>\n",
       "      <th>p_bar_9</th>\n",
       "      <th>p_bar_10</th>\n",
       "      <th>...</th>\n",
       "      <th>p_bar_21</th>\n",
       "      <th>p_bar_22</th>\n",
       "      <th>p_bar_23</th>\n",
       "      <th>p_bar_24</th>\n",
       "      <th>p_bar_25</th>\n",
       "      <th>p_bar_26</th>\n",
       "      <th>p_bar_27</th>\n",
       "      <th>p_bar_28</th>\n",
       "      <th>p_bar_29</th>\n",
       "      <th>p_bar_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>355.700520</td>\n",
       "      <td>993.352069</td>\n",
       "      <td>448.251782</td>\n",
       "      <td>49.615354</td>\n",
       "      <td>39.880775</td>\n",
       "      <td>861.371579</td>\n",
       "      <td>481.199777</td>\n",
       "      <td>629.292516</td>\n",
       "      <td>330.464351</td>\n",
       "      <td>940.928686</td>\n",
       "      <td>...</td>\n",
       "      <td>906.587488</td>\n",
       "      <td>547.259399</td>\n",
       "      <td>504.896821</td>\n",
       "      <td>930.298415</td>\n",
       "      <td>840.357279</td>\n",
       "      <td>454.571237</td>\n",
       "      <td>407.002246</td>\n",
       "      <td>622.795989</td>\n",
       "      <td>255.391495</td>\n",
       "      <td>723.444461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>451.245049</td>\n",
       "      <td>828.551112</td>\n",
       "      <td>500.348184</td>\n",
       "      <td>417.844589</td>\n",
       "      <td>889.071221</td>\n",
       "      <td>504.839585</td>\n",
       "      <td>550.074016</td>\n",
       "      <td>434.011981</td>\n",
       "      <td>987.160709</td>\n",
       "      <td>291.770814</td>\n",
       "      <td>...</td>\n",
       "      <td>425.839505</td>\n",
       "      <td>756.133285</td>\n",
       "      <td>771.896988</td>\n",
       "      <td>116.525823</td>\n",
       "      <td>760.526197</td>\n",
       "      <td>921.137421</td>\n",
       "      <td>503.677264</td>\n",
       "      <td>909.711514</td>\n",
       "      <td>411.812343</td>\n",
       "      <td>721.403019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>981.658293</td>\n",
       "      <td>383.325108</td>\n",
       "      <td>863.991257</td>\n",
       "      <td>697.432032</td>\n",
       "      <td>926.911016</td>\n",
       "      <td>686.134790</td>\n",
       "      <td>288.246217</td>\n",
       "      <td>697.917332</td>\n",
       "      <td>428.762568</td>\n",
       "      <td>293.282648</td>\n",
       "      <td>...</td>\n",
       "      <td>358.707533</td>\n",
       "      <td>624.684349</td>\n",
       "      <td>131.275083</td>\n",
       "      <td>100.557140</td>\n",
       "      <td>543.332195</td>\n",
       "      <td>631.620488</td>\n",
       "      <td>91.539625</td>\n",
       "      <td>549.677607</td>\n",
       "      <td>297.158315</td>\n",
       "      <td>190.495717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>272.231835</td>\n",
       "      <td>861.518957</td>\n",
       "      <td>760.206190</td>\n",
       "      <td>754.535561</td>\n",
       "      <td>271.581730</td>\n",
       "      <td>613.204899</td>\n",
       "      <td>525.153060</td>\n",
       "      <td>338.547404</td>\n",
       "      <td>755.578391</td>\n",
       "      <td>426.308419</td>\n",
       "      <td>...</td>\n",
       "      <td>142.348736</td>\n",
       "      <td>249.824057</td>\n",
       "      <td>556.234055</td>\n",
       "      <td>990.389523</td>\n",
       "      <td>490.080668</td>\n",
       "      <td>447.610723</td>\n",
       "      <td>374.467420</td>\n",
       "      <td>621.505147</td>\n",
       "      <td>398.149520</td>\n",
       "      <td>616.580119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>619.487643</td>\n",
       "      <td>630.951101</td>\n",
       "      <td>978.567517</td>\n",
       "      <td>568.807481</td>\n",
       "      <td>969.247187</td>\n",
       "      <td>647.795203</td>\n",
       "      <td>320.863978</td>\n",
       "      <td>284.979759</td>\n",
       "      <td>419.758544</td>\n",
       "      <td>310.364364</td>\n",
       "      <td>...</td>\n",
       "      <td>308.962467</td>\n",
       "      <td>486.936609</td>\n",
       "      <td>883.757275</td>\n",
       "      <td>665.768010</td>\n",
       "      <td>525.049157</td>\n",
       "      <td>453.565032</td>\n",
       "      <td>709.626002</td>\n",
       "      <td>762.306653</td>\n",
       "      <td>503.552278</td>\n",
       "      <td>873.383072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109995</th>\n",
       "      <td>406.974009</td>\n",
       "      <td>134.835375</td>\n",
       "      <td>221.696547</td>\n",
       "      <td>949.975132</td>\n",
       "      <td>524.460573</td>\n",
       "      <td>692.092738</td>\n",
       "      <td>79.673939</td>\n",
       "      <td>799.199671</td>\n",
       "      <td>852.728126</td>\n",
       "      <td>811.924969</td>\n",
       "      <td>...</td>\n",
       "      <td>542.180860</td>\n",
       "      <td>910.114067</td>\n",
       "      <td>612.059841</td>\n",
       "      <td>105.043770</td>\n",
       "      <td>205.106794</td>\n",
       "      <td>379.878876</td>\n",
       "      <td>41.327088</td>\n",
       "      <td>108.364379</td>\n",
       "      <td>284.911174</td>\n",
       "      <td>806.399765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109996</th>\n",
       "      <td>409.253690</td>\n",
       "      <td>187.868659</td>\n",
       "      <td>382.280181</td>\n",
       "      <td>438.395836</td>\n",
       "      <td>90.506140</td>\n",
       "      <td>983.559855</td>\n",
       "      <td>615.280176</td>\n",
       "      <td>433.334421</td>\n",
       "      <td>269.869480</td>\n",
       "      <td>790.341454</td>\n",
       "      <td>...</td>\n",
       "      <td>635.495003</td>\n",
       "      <td>705.257189</td>\n",
       "      <td>932.501732</td>\n",
       "      <td>149.921551</td>\n",
       "      <td>920.986371</td>\n",
       "      <td>851.515818</td>\n",
       "      <td>968.173504</td>\n",
       "      <td>42.388695</td>\n",
       "      <td>824.213505</td>\n",
       "      <td>678.313412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109997</th>\n",
       "      <td>300.621711</td>\n",
       "      <td>757.784155</td>\n",
       "      <td>995.033276</td>\n",
       "      <td>54.534988</td>\n",
       "      <td>426.567721</td>\n",
       "      <td>781.574635</td>\n",
       "      <td>162.549687</td>\n",
       "      <td>793.789982</td>\n",
       "      <td>722.199047</td>\n",
       "      <td>144.112877</td>\n",
       "      <td>...</td>\n",
       "      <td>842.102305</td>\n",
       "      <td>761.536460</td>\n",
       "      <td>498.388247</td>\n",
       "      <td>42.538096</td>\n",
       "      <td>451.302462</td>\n",
       "      <td>727.895623</td>\n",
       "      <td>61.076862</td>\n",
       "      <td>868.118344</td>\n",
       "      <td>627.368470</td>\n",
       "      <td>17.033234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109998</th>\n",
       "      <td>494.126779</td>\n",
       "      <td>581.907702</td>\n",
       "      <td>18.792506</td>\n",
       "      <td>306.733403</td>\n",
       "      <td>996.485297</td>\n",
       "      <td>850.778355</td>\n",
       "      <td>47.389366</td>\n",
       "      <td>510.455286</td>\n",
       "      <td>982.581856</td>\n",
       "      <td>993.294914</td>\n",
       "      <td>...</td>\n",
       "      <td>78.154029</td>\n",
       "      <td>271.103371</td>\n",
       "      <td>954.084608</td>\n",
       "      <td>78.562945</td>\n",
       "      <td>675.370756</td>\n",
       "      <td>686.864468</td>\n",
       "      <td>480.857602</td>\n",
       "      <td>590.540388</td>\n",
       "      <td>429.565716</td>\n",
       "      <td>411.500715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109999</th>\n",
       "      <td>500.722387</td>\n",
       "      <td>244.120823</td>\n",
       "      <td>719.784070</td>\n",
       "      <td>473.503056</td>\n",
       "      <td>265.468543</td>\n",
       "      <td>205.299618</td>\n",
       "      <td>479.808021</td>\n",
       "      <td>856.773120</td>\n",
       "      <td>21.163725</td>\n",
       "      <td>163.341011</td>\n",
       "      <td>...</td>\n",
       "      <td>706.482337</td>\n",
       "      <td>151.520258</td>\n",
       "      <td>736.695850</td>\n",
       "      <td>24.554113</td>\n",
       "      <td>577.613928</td>\n",
       "      <td>5.960684</td>\n",
       "      <td>693.988722</td>\n",
       "      <td>316.738289</td>\n",
       "      <td>48.935813</td>\n",
       "      <td>850.240360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           p_bar_1     p_bar_2     p_bar_3     p_bar_4     p_bar_5  \\\n",
       "0       355.700520  993.352069  448.251782   49.615354   39.880775   \n",
       "1       451.245049  828.551112  500.348184  417.844589  889.071221   \n",
       "2       981.658293  383.325108  863.991257  697.432032  926.911016   \n",
       "3       272.231835  861.518957  760.206190  754.535561  271.581730   \n",
       "4       619.487643  630.951101  978.567517  568.807481  969.247187   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "109995  406.974009  134.835375  221.696547  949.975132  524.460573   \n",
       "109996  409.253690  187.868659  382.280181  438.395836   90.506140   \n",
       "109997  300.621711  757.784155  995.033276   54.534988  426.567721   \n",
       "109998  494.126779  581.907702   18.792506  306.733403  996.485297   \n",
       "109999  500.722387  244.120823  719.784070  473.503056  265.468543   \n",
       "\n",
       "           p_bar_6     p_bar_7     p_bar_8     p_bar_9    p_bar_10  ...  \\\n",
       "0       861.371579  481.199777  629.292516  330.464351  940.928686  ...   \n",
       "1       504.839585  550.074016  434.011981  987.160709  291.770814  ...   \n",
       "2       686.134790  288.246217  697.917332  428.762568  293.282648  ...   \n",
       "3       613.204899  525.153060  338.547404  755.578391  426.308419  ...   \n",
       "4       647.795203  320.863978  284.979759  419.758544  310.364364  ...   \n",
       "...            ...         ...         ...         ...         ...  ...   \n",
       "109995  692.092738   79.673939  799.199671  852.728126  811.924969  ...   \n",
       "109996  983.559855  615.280176  433.334421  269.869480  790.341454  ...   \n",
       "109997  781.574635  162.549687  793.789982  722.199047  144.112877  ...   \n",
       "109998  850.778355   47.389366  510.455286  982.581856  993.294914  ...   \n",
       "109999  205.299618  479.808021  856.773120   21.163725  163.341011  ...   \n",
       "\n",
       "          p_bar_21    p_bar_22    p_bar_23    p_bar_24    p_bar_25  \\\n",
       "0       906.587488  547.259399  504.896821  930.298415  840.357279   \n",
       "1       425.839505  756.133285  771.896988  116.525823  760.526197   \n",
       "2       358.707533  624.684349  131.275083  100.557140  543.332195   \n",
       "3       142.348736  249.824057  556.234055  990.389523  490.080668   \n",
       "4       308.962467  486.936609  883.757275  665.768010  525.049157   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "109995  542.180860  910.114067  612.059841  105.043770  205.106794   \n",
       "109996  635.495003  705.257189  932.501732  149.921551  920.986371   \n",
       "109997  842.102305  761.536460  498.388247   42.538096  451.302462   \n",
       "109998   78.154029  271.103371  954.084608   78.562945  675.370756   \n",
       "109999  706.482337  151.520258  736.695850   24.554113  577.613928   \n",
       "\n",
       "          p_bar_26    p_bar_27    p_bar_28    p_bar_29    p_bar_30  \n",
       "0       454.571237  407.002246  622.795989  255.391495  723.444461  \n",
       "1       921.137421  503.677264  909.711514  411.812343  721.403019  \n",
       "2       631.620488   91.539625  549.677607  297.158315  190.495717  \n",
       "3       447.610723  374.467420  621.505147  398.149520  616.580119  \n",
       "4       453.565032  709.626002  762.306653  503.552278  873.383072  \n",
       "...            ...         ...         ...         ...         ...  \n",
       "109995  379.878876   41.327088  108.364379  284.911174  806.399765  \n",
       "109996  851.515818  968.173504   42.388695  824.213505  678.313412  \n",
       "109997  727.895623   61.076862  868.118344  627.368470   17.033234  \n",
       "109998  686.864468  480.857602  590.540388  429.565716  411.500715  \n",
       "109999    5.960684  693.988722  316.738289   48.935813  850.240360  \n",
       "\n",
       "[110000 rows x 30 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f58e30f2-234f-4bca-9613-96afffcec432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = instance_df.loc[: , instance_df.columns.str.startswith('f')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "095ede9d-a55d-4aaa-ad6d-b19ac31006ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>f_9</th>\n",
       "      <th>f_10</th>\n",
       "      <th>...</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>396.512196</td>\n",
       "      <td>1297.809600</td>\n",
       "      <td>613.718375</td>\n",
       "      <td>59.185197</td>\n",
       "      <td>47.946346</td>\n",
       "      <td>1056.401838</td>\n",
       "      <td>631.779698</td>\n",
       "      <td>893.372085</td>\n",
       "      <td>366.330347</td>\n",
       "      <td>1174.537895</td>\n",
       "      <td>...</td>\n",
       "      <td>1148.276296</td>\n",
       "      <td>602.791108</td>\n",
       "      <td>577.487959</td>\n",
       "      <td>1150.598624</td>\n",
       "      <td>1063.628843</td>\n",
       "      <td>631.597793</td>\n",
       "      <td>541.957660</td>\n",
       "      <td>838.179388</td>\n",
       "      <td>286.753118</td>\n",
       "      <td>801.872119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>623.628920</td>\n",
       "      <td>1081.592385</td>\n",
       "      <td>612.387397</td>\n",
       "      <td>498.180039</td>\n",
       "      <td>1191.312302</td>\n",
       "      <td>571.571492</td>\n",
       "      <td>767.911943</td>\n",
       "      <td>482.113584</td>\n",
       "      <td>1222.010147</td>\n",
       "      <td>339.076379</td>\n",
       "      <td>...</td>\n",
       "      <td>557.867766</td>\n",
       "      <td>1030.168603</td>\n",
       "      <td>1015.223277</td>\n",
       "      <td>143.195646</td>\n",
       "      <td>1000.014423</td>\n",
       "      <td>1300.502422</td>\n",
       "      <td>660.518773</td>\n",
       "      <td>1072.048198</td>\n",
       "      <td>515.032453</td>\n",
       "      <td>977.944366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1253.426596</td>\n",
       "      <td>556.424704</td>\n",
       "      <td>1199.241427</td>\n",
       "      <td>884.281025</td>\n",
       "      <td>1382.551467</td>\n",
       "      <td>1022.653514</td>\n",
       "      <td>393.083434</td>\n",
       "      <td>817.754841</td>\n",
       "      <td>522.139677</td>\n",
       "      <td>406.952118</td>\n",
       "      <td>...</td>\n",
       "      <td>489.971271</td>\n",
       "      <td>867.503398</td>\n",
       "      <td>191.314264</td>\n",
       "      <td>133.305591</td>\n",
       "      <td>690.440925</td>\n",
       "      <td>858.471809</td>\n",
       "      <td>109.984531</td>\n",
       "      <td>720.831460</td>\n",
       "      <td>402.678179</td>\n",
       "      <td>244.153171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>323.081912</td>\n",
       "      <td>1203.205674</td>\n",
       "      <td>1133.803548</td>\n",
       "      <td>874.054442</td>\n",
       "      <td>356.635014</td>\n",
       "      <td>886.925487</td>\n",
       "      <td>616.162509</td>\n",
       "      <td>486.314744</td>\n",
       "      <td>1013.968744</td>\n",
       "      <td>521.157475</td>\n",
       "      <td>...</td>\n",
       "      <td>179.868529</td>\n",
       "      <td>280.758600</td>\n",
       "      <td>806.853533</td>\n",
       "      <td>1313.871708</td>\n",
       "      <td>694.291537</td>\n",
       "      <td>657.230354</td>\n",
       "      <td>490.044163</td>\n",
       "      <td>817.036387</td>\n",
       "      <td>521.570367</td>\n",
       "      <td>684.186356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>702.421362</td>\n",
       "      <td>854.738524</td>\n",
       "      <td>1297.184654</td>\n",
       "      <td>651.214259</td>\n",
       "      <td>1336.018114</td>\n",
       "      <td>746.598288</td>\n",
       "      <td>410.188603</td>\n",
       "      <td>315.122844</td>\n",
       "      <td>496.577546</td>\n",
       "      <td>396.307285</td>\n",
       "      <td>...</td>\n",
       "      <td>438.806849</td>\n",
       "      <td>628.142642</td>\n",
       "      <td>1054.487544</td>\n",
       "      <td>870.338208</td>\n",
       "      <td>742.064592</td>\n",
       "      <td>650.210999</td>\n",
       "      <td>1030.728758</td>\n",
       "      <td>1009.716738</td>\n",
       "      <td>722.198458</td>\n",
       "      <td>1259.052669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109995</th>\n",
       "      <td>508.400908</td>\n",
       "      <td>156.740917</td>\n",
       "      <td>289.148614</td>\n",
       "      <td>1364.355644</td>\n",
       "      <td>599.891544</td>\n",
       "      <td>971.933136</td>\n",
       "      <td>111.605846</td>\n",
       "      <td>883.847914</td>\n",
       "      <td>1084.300412</td>\n",
       "      <td>978.257699</td>\n",
       "      <td>...</td>\n",
       "      <td>637.931515</td>\n",
       "      <td>1250.684956</td>\n",
       "      <td>678.994123</td>\n",
       "      <td>116.925166</td>\n",
       "      <td>233.645120</td>\n",
       "      <td>443.261377</td>\n",
       "      <td>52.680572</td>\n",
       "      <td>140.263440</td>\n",
       "      <td>389.068969</td>\n",
       "      <td>1160.758085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109996</th>\n",
       "      <td>591.717100</td>\n",
       "      <td>214.295276</td>\n",
       "      <td>533.906373</td>\n",
       "      <td>614.167505</td>\n",
       "      <td>121.902593</td>\n",
       "      <td>1267.037622</td>\n",
       "      <td>683.391050</td>\n",
       "      <td>540.649419</td>\n",
       "      <td>324.659635</td>\n",
       "      <td>908.013121</td>\n",
       "      <td>...</td>\n",
       "      <td>805.499629</td>\n",
       "      <td>936.237489</td>\n",
       "      <td>1230.463844</td>\n",
       "      <td>217.480977</td>\n",
       "      <td>1292.851605</td>\n",
       "      <td>1169.187127</td>\n",
       "      <td>1228.142900</td>\n",
       "      <td>63.482305</td>\n",
       "      <td>1125.518741</td>\n",
       "      <td>761.350182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109997</th>\n",
       "      <td>330.726783</td>\n",
       "      <td>847.939582</td>\n",
       "      <td>1313.493760</td>\n",
       "      <td>77.294263</td>\n",
       "      <td>562.768650</td>\n",
       "      <td>1165.494092</td>\n",
       "      <td>226.612155</td>\n",
       "      <td>1123.066087</td>\n",
       "      <td>950.502763</td>\n",
       "      <td>192.180943</td>\n",
       "      <td>...</td>\n",
       "      <td>1061.196682</td>\n",
       "      <td>1000.040399</td>\n",
       "      <td>664.810709</td>\n",
       "      <td>57.744120</td>\n",
       "      <td>614.965444</td>\n",
       "      <td>1060.948272</td>\n",
       "      <td>71.407806</td>\n",
       "      <td>1298.210960</td>\n",
       "      <td>758.184912</td>\n",
       "      <td>20.711536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109998</th>\n",
       "      <td>562.489980</td>\n",
       "      <td>806.524818</td>\n",
       "      <td>25.785823</td>\n",
       "      <td>364.639071</td>\n",
       "      <td>1252.327599</td>\n",
       "      <td>1039.483778</td>\n",
       "      <td>63.860173</td>\n",
       "      <td>640.240759</td>\n",
       "      <td>1342.227796</td>\n",
       "      <td>1235.882654</td>\n",
       "      <td>...</td>\n",
       "      <td>92.209535</td>\n",
       "      <td>393.851605</td>\n",
       "      <td>1353.231665</td>\n",
       "      <td>101.899278</td>\n",
       "      <td>919.957596</td>\n",
       "      <td>1019.198237</td>\n",
       "      <td>607.587580</td>\n",
       "      <td>719.001927</td>\n",
       "      <td>497.834304</td>\n",
       "      <td>473.371679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109999</th>\n",
       "      <td>553.207509</td>\n",
       "      <td>291.906703</td>\n",
       "      <td>935.592371</td>\n",
       "      <td>684.166461</td>\n",
       "      <td>378.733074</td>\n",
       "      <td>255.413867</td>\n",
       "      <td>643.535161</td>\n",
       "      <td>1268.007301</td>\n",
       "      <td>30.814349</td>\n",
       "      <td>208.477737</td>\n",
       "      <td>...</td>\n",
       "      <td>837.362950</td>\n",
       "      <td>214.425681</td>\n",
       "      <td>1026.133173</td>\n",
       "      <td>32.111331</td>\n",
       "      <td>750.312706</td>\n",
       "      <td>8.546047</td>\n",
       "      <td>905.091307</td>\n",
       "      <td>423.610628</td>\n",
       "      <td>65.119335</td>\n",
       "      <td>1195.759049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                f_1          f_2          f_3          f_4          f_5  \\\n",
       "0        396.512196  1297.809600   613.718375    59.185197    47.946346   \n",
       "1        623.628920  1081.592385   612.387397   498.180039  1191.312302   \n",
       "2       1253.426596   556.424704  1199.241427   884.281025  1382.551467   \n",
       "3        323.081912  1203.205674  1133.803548   874.054442   356.635014   \n",
       "4        702.421362   854.738524  1297.184654   651.214259  1336.018114   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "109995   508.400908   156.740917   289.148614  1364.355644   599.891544   \n",
       "109996   591.717100   214.295276   533.906373   614.167505   121.902593   \n",
       "109997   330.726783   847.939582  1313.493760    77.294263   562.768650   \n",
       "109998   562.489980   806.524818    25.785823   364.639071  1252.327599   \n",
       "109999   553.207509   291.906703   935.592371   684.166461   378.733074   \n",
       "\n",
       "                f_6         f_7          f_8          f_9         f_10  ...  \\\n",
       "0       1056.401838  631.779698   893.372085   366.330347  1174.537895  ...   \n",
       "1        571.571492  767.911943   482.113584  1222.010147   339.076379  ...   \n",
       "2       1022.653514  393.083434   817.754841   522.139677   406.952118  ...   \n",
       "3        886.925487  616.162509   486.314744  1013.968744   521.157475  ...   \n",
       "4        746.598288  410.188603   315.122844   496.577546   396.307285  ...   \n",
       "...             ...         ...          ...          ...          ...  ...   \n",
       "109995   971.933136  111.605846   883.847914  1084.300412   978.257699  ...   \n",
       "109996  1267.037622  683.391050   540.649419   324.659635   908.013121  ...   \n",
       "109997  1165.494092  226.612155  1123.066087   950.502763   192.180943  ...   \n",
       "109998  1039.483778   63.860173   640.240759  1342.227796  1235.882654  ...   \n",
       "109999   255.413867  643.535161  1268.007301    30.814349   208.477737  ...   \n",
       "\n",
       "               f_21         f_22         f_23         f_24         f_25  \\\n",
       "0       1148.276296   602.791108   577.487959  1150.598624  1063.628843   \n",
       "1        557.867766  1030.168603  1015.223277   143.195646  1000.014423   \n",
       "2        489.971271   867.503398   191.314264   133.305591   690.440925   \n",
       "3        179.868529   280.758600   806.853533  1313.871708   694.291537   \n",
       "4        438.806849   628.142642  1054.487544   870.338208   742.064592   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "109995   637.931515  1250.684956   678.994123   116.925166   233.645120   \n",
       "109996   805.499629   936.237489  1230.463844   217.480977  1292.851605   \n",
       "109997  1061.196682  1000.040399   664.810709    57.744120   614.965444   \n",
       "109998    92.209535   393.851605  1353.231665   101.899278   919.957596   \n",
       "109999   837.362950   214.425681  1026.133173    32.111331   750.312706   \n",
       "\n",
       "               f_26         f_27         f_28         f_29         f_30  \n",
       "0        631.597793   541.957660   838.179388   286.753118   801.872119  \n",
       "1       1300.502422   660.518773  1072.048198   515.032453   977.944366  \n",
       "2        858.471809   109.984531   720.831460   402.678179   244.153171  \n",
       "3        657.230354   490.044163   817.036387   521.570367   684.186356  \n",
       "4        650.210999  1030.728758  1009.716738   722.198458  1259.052669  \n",
       "...             ...          ...          ...          ...          ...  \n",
       "109995   443.261377    52.680572   140.263440   389.068969  1160.758085  \n",
       "109996  1169.187127  1228.142900    63.482305  1125.518741   761.350182  \n",
       "109997  1060.948272    71.407806  1298.210960   758.184912    20.711536  \n",
       "109998  1019.198237   607.587580   719.001927   497.834304   473.371679  \n",
       "109999     8.546047   905.091307   423.610628    65.119335  1195.759049  \n",
       "\n",
       "[110000 rows x 30 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff947f79-a697-462b-b11b-d532f0f0bff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_df=split_list_column(combined_data, \"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d3a6093-745b-4db4-a823-bcb06aad39b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110000, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "222f9bff-70a3-46bd-8d24-cde0c3efacc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Uncer_df=split_list_column(combined_data, \"uncern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66fe6873-3e6b-4495-83cd-8ce39869902b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110000, 30)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Uncer_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5ae660b-c870-4d19-aebe-69434a79f57f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### Master problem objective\n",
    "######  sum_i_((f_i - p_bar_i) - * x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "780834d5-fda1-4bd4-9892-02bb107a1112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4561.28661058, 3866.58443701, 3220.07389161, ..., 4694.17634219,\n",
       "       4356.5687916 , 3573.82450646])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((f.values - p_bar.values) * X_df.values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfef10eb-12f3-4f11-af81-73ccb532b293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f', 'x', 'r', 'second_stage_obj', 'First stage value', 'p_bar',\n",
       "       'Reduced Capacity', 't', 'p_hat', 'y', 'original_capacity', 'w',\n",
       "       'uncern', 'gamma', 'seed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ceb9fc6-edae-460d-b1e3-e6f454b0f2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        -20095.336990\n",
       "1        -16263.838092\n",
       "2        -13137.218650\n",
       "3        -18684.906823\n",
       "4        -18641.801475\n",
       "              ...     \n",
       "109995   -12835.288650\n",
       "109996   -15343.637301\n",
       "109997   -18573.031778\n",
       "109998   -19756.175924\n",
       "109999   -14058.097285\n",
       "Name: second_stage_obj, Length: 110000, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data[\"second_stage_obj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2531c068-8a30-4124-b895-f39df24402f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### Very important line, always check!!!! This only gives the second stage values of the loss\n",
    "target=combined_data[\"second_stage_obj\"] + np.sum((f.values - p_bar.values) * X_df.values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0ba6cfe-65b2-4c0f-ab20-706a6b93e6fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        -15534.050379\n",
       "1        -12397.253655\n",
       "2         -9917.144758\n",
       "3        -14145.234771\n",
       "4        -14353.283008\n",
       "              ...     \n",
       "109995    -9885.430475\n",
       "109996   -11713.580727\n",
       "109997   -13878.855435\n",
       "109998   -15399.607133\n",
       "109999   -10484.272779\n",
       "Name: second_stage_obj, Length: 110000, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786894f-799e-44b4-a9d2-ebdc137f94d0",
   "metadata": {},
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb7dd441-1e40-4940-9aec-378f6de6adfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-15534.0503793 , -12397.25365485,  -9917.14475847, ...,\n",
       "       -13878.85543537, -15399.60713263, -10484.27277896])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert target column to numpy array\n",
    "target_array = target.values\n",
    "target_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b8f250f-307d-4249-b737-f372bac1c63b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6500.939944613325"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_array.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad52d486-eeec-4ec5-80ef-06e7b2a3833a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 30, 30)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_df.shape[1],X_df.shape[1],Uncer_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dfd1880-fda2-468e-9199-f0fcd879f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, instance_size=152, X_size=30, Uncern_size=30):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding_instance = nn.Linear(instance_size, 64)\n",
    "        self.embedding_X = nn.Linear(X_size, 15)\n",
    "        self.embedding_uncern = nn.Linear(Uncern_size, 15)\n",
    "        self.fc1 = nn.Linear(94, 128)  \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64,30)\n",
    "        self.fc4= nn.Linear(30,10)\n",
    "        self.fc5= nn.Linear(10,1)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, instance, X, uncern):\n",
    "        \n",
    "        instance_embed = torch.relu(self.embedding_instance(instance))\n",
    "        X_embed = torch.relu(self.embedding_X(X))\n",
    "        uncern_embed = torch.relu(self.embedding_uncern(uncern))\n",
    "        \n",
    "        concatenated = torch.cat((instance_embed, X_embed, uncern_embed), dim=1)\n",
    "        \n",
    "        output = torch.relu(self.fc1(concatenated))\n",
    "        output = torch.relu(self.fc2(output))\n",
    "        output = torch.relu(self.fc3(output))\n",
    "        output = torch.relu(self.fc4(output))\n",
    "        output = self.fc5(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "629baedd-529e-4757-921b-72fd0a7c50bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "\n",
    "# Access fc1 layer\n",
    "fc1_layer = model.fc1\n",
    "\n",
    "# Get the named children of fc1 layer\n",
    "fc1_children = list(fc1_layer.named_children())\n",
    "\n",
    "# Print the named children\n",
    "print(fc1_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5db253a-fb9c-4a0a-9d91-bfa516bdfe2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0677,  0.0529, -0.0459,  ..., -0.0929,  0.0844,  0.0959],\n",
      "        [-0.0004, -0.0804,  0.0958,  ..., -0.0500,  0.0868,  0.0470],\n",
      "        [ 0.0457, -0.0456,  0.0348,  ..., -0.0623,  0.0386, -0.0677],\n",
      "        ...,\n",
      "        [ 0.0781,  0.0374,  0.0920,  ...,  0.0997,  0.0073, -0.0411],\n",
      "        [-0.0313, -0.0489,  0.0265,  ..., -0.0956,  0.0225,  0.0182],\n",
      "        [ 0.0708, -0.0146, -0.0173,  ...,  0.0557,  0.0405, -0.0254]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 5.9467e-02, -8.8047e-02,  5.5949e-02, -8.4145e-02,  3.6073e-02,\n",
      "        -4.6981e-02,  5.9066e-02,  3.3266e-02,  8.7923e-02, -3.6221e-02,\n",
      "         5.6682e-02, -7.3006e-02, -8.7046e-02, -1.0283e-01,  7.0124e-02,\n",
      "        -1.8964e-02,  8.9428e-02,  2.4599e-02, -4.1385e-02,  9.3615e-02,\n",
      "        -1.4957e-02,  5.2630e-02, -4.3941e-02,  4.4552e-03, -6.9760e-02,\n",
      "         3.8948e-02, -1.0104e-01, -3.1709e-02, -3.6666e-02,  7.3714e-02,\n",
      "         2.2916e-02,  5.1897e-02,  7.1406e-02, -2.8822e-02, -3.7826e-02,\n",
      "         3.3998e-02, -6.2771e-04, -7.6668e-02, -6.6293e-02,  6.6312e-02,\n",
      "         9.9863e-02, -5.9264e-03, -9.8544e-02, -2.1753e-02, -9.1391e-02,\n",
      "         1.0153e-01, -5.1110e-02, -7.6366e-02, -9.1274e-02,  6.6808e-02,\n",
      "         2.4793e-02,  2.4193e-02, -1.5800e-02, -5.6184e-03,  7.8306e-02,\n",
      "         9.6779e-02,  7.5945e-02,  3.7440e-02,  5.1382e-02,  5.4247e-02,\n",
      "         6.5378e-02, -1.7448e-02,  1.5219e-02,  6.2507e-02,  7.4196e-02,\n",
      "         1.8815e-02,  4.0423e-02,  5.4001e-02,  8.2074e-02,  4.6586e-02,\n",
      "        -2.1400e-03, -6.4764e-02, -8.6438e-02, -1.7985e-02, -2.6408e-02,\n",
      "        -6.7299e-02, -4.4231e-02,  7.5521e-02,  6.7097e-02, -8.7612e-02,\n",
      "        -1.5099e-02, -6.5890e-02,  8.4057e-02, -9.1595e-02,  8.3782e-02,\n",
      "         3.9416e-02,  8.4613e-02,  3.9161e-02, -1.6758e-02,  3.5552e-02,\n",
      "         7.6378e-02,  7.6739e-02, -3.6565e-02, -4.9505e-02,  8.5777e-02,\n",
      "         8.6488e-02,  5.0961e-02, -1.8960e-02,  6.6200e-02, -6.3737e-02,\n",
      "         1.9140e-02, -5.5562e-02, -1.9044e-02,  9.9445e-02,  1.8734e-02,\n",
      "        -7.5764e-03,  5.7188e-02,  4.6149e-02, -1.9254e-02,  7.0951e-03,\n",
      "         1.7644e-05,  8.3801e-02,  7.7779e-02,  6.7478e-02, -7.4569e-02,\n",
      "         3.6136e-02, -7.2989e-02, -4.4060e-03,  1.0201e-01,  1.2051e-03,\n",
      "        -1.1205e-02, -4.9139e-02, -1.9889e-02,  8.0902e-02,  2.5651e-02,\n",
      "         9.3470e-02,  5.9613e-02, -6.8018e-02], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "fc1_params = list(fc1_layer.parameters())\n",
    "\n",
    "# Print the parameters\n",
    "print(fc1_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8cab80-d882-4860-bfa5-cfe5a3775475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812be2e-26a7-4057-9848-f6db2424932b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc525661-e7ac-4250-ae20-5f7454120e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47344739-fa2b-4af9-8c76-6a04b4f1ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "instance_train, instance_test, \\\n",
    "X_train, X_test, \\\n",
    "uncern_train, uncern_test, \\\n",
    "target_train, target_test = train_test_split(instance_df.values, X_df.values, Uncer_df.values, target_array, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231c0af-98fb-474c-904b-bc842aa766b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e54cbc1-0dff-4cce-9720-3ec7fa8e00dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Since the X_train is always scaled between 0 or 1, here we shall just scale the values of the intsnace_train and uncern_train. Once this is achieved, we will get the values of x_min and x_max from the training set which we wil further use to scale the values of the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6da62910-d89d-47ac-af57-2154c1aa65f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler_instance = MinMaxScaler()\n",
    "scaler_instance.fit(instance_train)\n",
    "instance_train_transformed = scaler_instance.transform(instance_train)\n",
    "instance_test_transformed = scaler_instance.transform(instance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cae7b4b-b392-4868-830b-ac7f7dab1edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler_uncern = MinMaxScaler()\n",
    "scaler_uncern.fit(uncern_train)\n",
    "uncern_train_transformed = scaler_uncern.transform(uncern_train)\n",
    "uncern_test_transformed = scaler_uncern.transform(uncern_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "068cafd7-fb11-4ffa-a060-73b89248636a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler_target = MinMaxScaler()\n",
    "scaler_target.fit(target_train.reshape(-1,1))\n",
    "target_train_transformed = scaler_target.transform(target_train.reshape(-1,1))\n",
    "target_test_transformed = scaler_target.transform(target_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55643f4c-fe63-4803-a6de-08bb2e401480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6527.5427947])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_target.data_max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1139fd7-4dc8-4545-96aa-b5f7f6ec385d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-18754.54910045])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_target.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b77128ff-f3d6-453d-9dc0-53e6c34f796c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created successfully!\n"
     ]
    }
   ],
   "source": [
    "min_max_scalers={}\n",
    "min_max_scalers[\"scaler_instance.data_max_\"]=scaler_instance.data_max_.tolist()\n",
    "min_max_scalers[\"scaler_instance.data_min_\"]=scaler_instance.data_min_.tolist()\n",
    "min_max_scalers[\"scaler_uncern.data_max_\"]=scaler_uncern.data_max_.tolist()\n",
    "min_max_scalers[\"scaler_uncern.data_min_\"]=scaler_uncern.data_min_.tolist()\n",
    "min_max_scalers[\"scaler_target.data_max_\"]=scaler_target.data_max_.tolist()\n",
    "min_max_scalers[\"scaler_target.data_min_\"]=scaler_target.data_min_.tolist()\n",
    "\n",
    "# Specify the file path\n",
    "\n",
    "file = \"min_max_scalers_inst_30.json\"\n",
    "file_path =  os.path.join(path_save, file)\n",
    "# Write dictionary to JSON file\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(min_max_scalers, json_file)\n",
    "\n",
    "print(\"JSON file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55c11246-abe4-4492-b2af-2fa2eec0bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "instance_train_tensor = torch.tensor(instance_train_transformed, dtype=torch.float32)\n",
    "instance_test_tensor = torch.tensor(instance_test_transformed, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "uncern_train_tensor = torch.tensor(uncern_train_transformed, dtype=torch.float32)\n",
    "uncern_test_tensor = torch.tensor(uncern_test_transformed, dtype=torch.float32)\n",
    "target_train_tensor = torch.tensor(target_train_transformed, dtype=torch.float32)\n",
    "target_test_tensor = torch.tensor(target_test_transformed, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a9a40-57d7-43d2-8312-54e0c0ee831b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b43d4d91-f305-4d86-a876-525eca11a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test datasets\n",
    "train_dataset = TensorDataset(instance_train_tensor, X_train_tensor, uncern_train_tensor, target_train_tensor)\n",
    "test_dataset = TensorDataset(instance_test_tensor, X_test_tensor, uncern_test_tensor, target_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ae03941-6a9c-4c86-9567-32ecf0a68c15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a2385fe-4f3c-4d3d-a3f1-2fea85c3b35d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Move model to GPU\n",
    "model = MyModel().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# Create training and test data loaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27d2bbe5-4314-4216-a18e-854057928c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] epoch_loss: 0.000151 batch_loss: 0.000023 batch_mae: 0.003598\n",
      "True MAE: 43.998329 , True MAPE: 0.398016 \n",
      "[1] Training MAE: 0.004490 ***Validation*** MAE: 0.004287 #######  True MAE on test batch 50.532 , True MAPE on test batch 0.464\n",
      "Epoch 00033: reducing learning rate of group 0 to 9.0000e-03.\n",
      "Epoch 00064: reducing learning rate of group 0 to 8.1000e-03.\n",
      "Epoch 00095: reducing learning rate of group 0 to 7.2900e-03.\n",
      "[101,   300] epoch_loss: 0.000081 batch_loss: 0.000020 batch_mae: 0.003305\n",
      "True MAE: 40.411575 , True MAPE: 0.371967 \n",
      "[101] Training MAE: 0.003921 ***Validation*** MAE: 0.004319 #######  True MAE on test batch 49.472 , True MAPE on test batch 0.451\n",
      "Epoch 00126: reducing learning rate of group 0 to 6.5610e-03.\n",
      "Epoch 00180: reducing learning rate of group 0 to 5.9049e-03.\n",
      "[201,   300] epoch_loss: 0.000074 batch_loss: 0.000029 batch_mae: 0.004077\n",
      "True MAE: 49.851288 , True MAPE: 0.460420 \n",
      "[201] Training MAE: 0.003776 ***Validation*** MAE: 0.004472 #######  True MAE on test batch 44.686 , True MAPE on test batch 0.389\n",
      "Epoch 00254: reducing learning rate of group 0 to 5.3144e-03.\n",
      "Epoch 00285: reducing learning rate of group 0 to 4.7830e-03.\n",
      "[301,   300] epoch_loss: 0.000070 batch_loss: 0.000025 batch_mae: 0.004008\n",
      "True MAE: 49.003559 , True MAPE: 0.458753 \n",
      "[301] Training MAE: 0.003651 ***Validation*** MAE: 0.004192 #######  True MAE on test batch 46.860 , True MAPE on test batch 0.428\n",
      "Epoch 00335: reducing learning rate of group 0 to 4.3047e-03.\n",
      "Epoch 00378: reducing learning rate of group 0 to 3.8742e-03.\n",
      "[401,   300] epoch_loss: 0.000067 batch_loss: 0.000022 batch_mae: 0.003548\n",
      "True MAE: 43.383720 , True MAPE: 0.388067 \n",
      "[401] Training MAE: 0.003537 ***Validation*** MAE: 0.004268 #######  True MAE on test batch 40.539 , True MAPE on test batch 0.369\n",
      "Epoch 00427: reducing learning rate of group 0 to 3.4868e-03.\n",
      "Epoch 00458: reducing learning rate of group 0 to 3.1381e-03.\n",
      "[501,   300] epoch_loss: 0.000063 batch_loss: 0.000019 batch_mae: 0.003021\n",
      "True MAE: 36.937347 , True MAPE: 0.333117 \n",
      "[501] Training MAE: 0.003437 ***Validation*** MAE: 0.004125 #######  True MAE on test batch 43.527 , True MAPE on test batch 0.391\n",
      "Epoch 00524: reducing learning rate of group 0 to 2.8243e-03.\n",
      "Epoch 00555: reducing learning rate of group 0 to 2.5419e-03.\n",
      "Epoch 00590: reducing learning rate of group 0 to 2.2877e-03.\n",
      "[601,   300] epoch_loss: 0.000060 batch_loss: 0.000022 batch_mae: 0.003644\n",
      "True MAE: 44.556526 , True MAPE: 0.398761 \n",
      "[601] Training MAE: 0.003341 ***Validation*** MAE: 0.004197 #######  True MAE on test batch 40.243 , True MAPE on test batch 0.379\n",
      "Epoch 00621: reducing learning rate of group 0 to 2.0589e-03.\n",
      "Epoch 00664: reducing learning rate of group 0 to 1.8530e-03.\n",
      "Epoch 00695: reducing learning rate of group 0 to 1.6677e-03.\n",
      "[701,   300] epoch_loss: 0.000058 batch_loss: 0.000015 batch_mae: 0.002860\n",
      "True MAE: 34.963379 , True MAPE: 0.325471 \n",
      "[701] Training MAE: 0.003257 ***Validation*** MAE: 0.004038 #######  True MAE on test batch 38.266 , True MAPE on test batch 0.360\n",
      "Epoch 00726: reducing learning rate of group 0 to 1.5009e-03.\n",
      "Epoch 00757: reducing learning rate of group 0 to 1.3509e-03.\n",
      "[801,   300] epoch_loss: 0.000057 batch_loss: 0.000018 batch_mae: 0.003296\n",
      "True MAE: 40.297768 , True MAPE: 0.361982 \n",
      "[801] Training MAE: 0.003218 ***Validation*** MAE: 0.004082 #######  True MAE on test batch 38.138 , True MAPE on test batch 0.360\n",
      "Epoch 00827: reducing learning rate of group 0 to 1.2158e-03.\n",
      "Epoch 00858: reducing learning rate of group 0 to 1.0942e-03.\n",
      "Epoch 00889: reducing learning rate of group 0 to 9.8477e-04.\n",
      "[901,   300] epoch_loss: 0.000055 batch_loss: 0.000017 batch_mae: 0.002957\n",
      "True MAE: 36.153351 , True MAPE: 0.331395 \n",
      "[901] Training MAE: 0.003165 ***Validation*** MAE: 0.003993 #######  True MAE on test batch 37.437 , True MAPE on test batch 0.344\n",
      "Epoch 00920: reducing learning rate of group 0 to 8.8629e-04.\n",
      "Epoch 00951: reducing learning rate of group 0 to 7.9766e-04.\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adamax(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=30, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_mae = 0.0  # Initialize running MAE\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs_instance, inputs_X, inputs_uncern, labels = data\n",
    "        # Move tensors to GPU\n",
    "        inputs_instance, inputs_X, inputs_uncern, labels = inputs_instance.to(device), inputs_X.to(device), inputs_uncern.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs_instance, inputs_X, inputs_uncern)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))  # Assuming labels is a column vector\n",
    "        \n",
    "        # Calculate Mean Absolute Error (MAE)\n",
    "        mae = torch.mean(torch.abs(outputs - labels.view(-1, 1)))\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_mae += mae.item()  # Accumulate MAE\n",
    "        \n",
    "        ######## Print the true loss on the inverse transformed variables\n",
    "        \n",
    "        ############################################################################\n",
    "        outputs_invtransformed = scaler_target.inverse_transform(outputs.cpu().detach().numpy())\n",
    "        labels_invtransformed = scaler_target.inverse_transform(labels.view(-1, 1).cpu().detach().numpy())\n",
    "        outputs_invtransformed = outputs_invtransformed.flatten()\n",
    "        labels_invtransformed = labels_invtransformed.flatten()\n",
    "        # Calculate MSE\n",
    "        mae_invtransformed = np.mean(np.abs(outputs_invtransformed - labels_invtransformed))\n",
    "        # Calculate MAPE\n",
    "        mape_invtransformed = np.mean(np.abs((outputs_invtransformed - labels_invtransformed) /labels_invtransformed)) * 100\n",
    "        #############################################################################\n",
    "        \n",
    "        \n",
    "        if epoch % 100 == 0 and (i + 1) % 300 == 0:  # Print every 100 epochs and 100th mini-batch\n",
    "            print('[%d, %5d] epoch_loss: %.6f batch_loss: %.6f batch_mae: %.6f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100, loss.item(), mae.item()))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            print('True MAE: %.6f , True MAPE: %.6f '%\n",
    "                 (mae_invtransformed, mape_invtransformed))\n",
    "    \n",
    "        ############################################################################\n",
    "    \n",
    "    # Perform validation and adjust learning rate\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        test_mae = 0.0\n",
    "        mae_invtransformed_test = 0.0\n",
    "        mape_invtransformed_test = 0.0\n",
    "        for data in test_loader:\n",
    "            inputs_instance, inputs_X, inputs_uncern, labels = data\n",
    "            # Move tensors to GPU\n",
    "            inputs_instance, inputs_X, inputs_uncern, labels = inputs_instance.to(device), inputs_X.to(device), inputs_uncern.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs_instance, inputs_X, inputs_uncern)\n",
    "            test_loss += criterion(outputs, labels.view(-1, 1)).item()\n",
    "            test_mae += torch.mean(torch.abs(outputs - labels.view(-1, 1))).item()  # Calculate MAE for test set\n",
    "            \n",
    "            ####################################################################################\n",
    "            outputs_invtransformed_test = scaler_target.inverse_transform(outputs.cpu().detach().numpy())\n",
    "            labels_invtransformed_test = scaler_target.inverse_transform(labels.view(-1, 1).cpu().detach().numpy())\n",
    "            outputs_invtransformed_test = outputs_invtransformed.flatten()\n",
    "            labels_invtransformed_test = labels_invtransformed.flatten()\n",
    "            # Calculate MSE\n",
    "            mae_invtransformed_test += np.mean(np.abs(outputs_invtransformed_test - labels_invtransformed_test))\n",
    "            # Calculate MAPE\n",
    "            mape_invtransformed_test += np.mean(np.abs((outputs_invtransformed_test - labels_invtransformed_test) /labels_invtransformed_test)) * 100\n",
    "            \n",
    "            ####################################################################################\n",
    "            \n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "    if epoch % 100 == 0:  # Print every 100 epochs\n",
    "        print('[%d] Training MAE: %.6f ***Validation*** MAE: %.6f #######  True MAE on test batch %.3f , True MAPE on test batch %.3f' % (epoch + 1, running_mae / len(train_loader), test_mae / len(test_loader), mae_invtransformed_test/len(test_loader), mape_invtransformed_test/len(test_loader)))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14558595-9687-4a8e-abad-da3cc03888c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (embedding_instance): Linear(in_features=152, out_features=64, bias=True)\n",
       "  (embedding_X): Linear(in_features=30, out_features=15, bias=True)\n",
       "  (embedding_uncern): Linear(in_features=30, out_features=15, bias=True)\n",
       "  (fc1): Linear(in_features=94, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=30, bias=True)\n",
       "  (fc4): Linear(in_features=30, out_features=10, bias=True)\n",
       "  (fc5): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87feac63-fb8a-4a9e-9992-ab03ff721c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved successfully at: /ztank/scratch/user/u.rd143338/ss_from_nn/Neural_second_stage/instance 30 scenario 50/model_weights_30_instance_50_scen.pth\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path where you want to save the model weights\n",
    "\n",
    "file = \"model_weights_30_instance_50_scen.pth\"\n",
    "file_path = os.path.join(path_save, file)\n",
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), file_path)\n",
    "\n",
    "print(\"Model weights saved successfully at:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "816342ba-6dae-4b4f-865c-3faf140eded1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_weights_biases_to_json(model, path_save, 'model_weight_30_instance_50_scen.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a93807fc-a94b-4b00-854b-1874a0b369b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_instance.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45910dfb-d297-4363-b150-2b51d53e8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store dictionaries of data\n",
    "data_dicts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs_instance, inputs_X, inputs_uncern, labels = data\n",
    "        inputs_instance, inputs_X, inputs_uncern, labels = inputs_instance.to(device), inputs_X.to(device), inputs_uncern.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs_instance, inputs_X, inputs_uncern).cpu().numpy()  # Convert predictions to numpy array\n",
    "        labels = labels.cpu().numpy()  # Convert labels to numpy array\n",
    "        \n",
    "        # Iterate through batch\n",
    "        for i in range(len(labels)):\n",
    "            # Prepare a dictionary for row data\n",
    "            row_data = {}\n",
    "            for j in range(inputs_instance.shape[1]):\n",
    "                row_data['Input_Instance_' + str(j)] = inputs_instance[i][j].item()\n",
    "            for j in range(inputs_X.shape[1]):\n",
    "                row_data['Input_X_' + str(j)] = inputs_X[i][j].item()\n",
    "                row_data['Input_Uncern_' + str(j)] = inputs_uncern[i][j].item()\n",
    "            row_data['Predicted_Output'] = outputs[i][0]\n",
    "            row_data['Real_Output'] = labels[i]\n",
    "            \n",
    "            # Append row dictionary to the list\n",
    "            data_dicts.append(row_data)\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "test_data_df = pd.DataFrame(data_dicts)\n",
    "test_data_df = pd.concat([test_data_df.iloc[:,:inputs_instance.shape[1]], test_data_df[sorted(test_data_df.iloc[:,inputs_instance.shape[1]:])]], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "239a4a8b-4285-418b-8738-9b884b0b09d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list to store dictionaries of data\n",
    "data_dicts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs_instance, inputs_X, inputs_uncern, labels = data\n",
    "        inputs_instance, inputs_X, inputs_uncern, labels = inputs_instance.to(device), inputs_X.to(device), inputs_uncern.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs_instance, inputs_X, inputs_uncern).cpu().numpy()  # Convert predictions to numpy array\n",
    "        labels = labels.cpu().numpy()  # Convert labels to numpy array\n",
    "        \n",
    "        # Iterate through batch\n",
    "        for i in range(len(labels)):\n",
    "            # Prepare a dictionary for row data\n",
    "            row_data = {}\n",
    "            for j in range(inputs_instance.shape[1]):\n",
    "                row_data['Input_Instance_' + str(j)] = inputs_instance[i][j].item()\n",
    "            for j in range(inputs_X.shape[1]):\n",
    "                row_data['Input_X_' + str(j)] = inputs_X[i][j].item()\n",
    "                row_data['Input_Uncern_' + str(j)] = inputs_uncern[i][j].item()\n",
    "            row_data['Predicted_Output'] = outputs[i][0]\n",
    "            row_data['Real_Output'] = labels[i]\n",
    "            \n",
    "            # Append row dictionary to the list\n",
    "            data_dicts.append(row_data)\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "train_data_df = pd.DataFrame(data_dicts)\n",
    "train_data_df = pd.concat([train_data_df.iloc[:,:inputs_instance.shape[1]], train_data_df[sorted(train_data_df.iloc[:,inputs_instance.shape[1]:])]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8c01bc4-3929-42ff-a5bc-b48851fa6f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input_Instance_0</th>\n",
       "      <th>Input_Instance_1</th>\n",
       "      <th>Input_Instance_2</th>\n",
       "      <th>Input_Instance_3</th>\n",
       "      <th>Input_Instance_4</th>\n",
       "      <th>Input_Instance_5</th>\n",
       "      <th>Input_Instance_6</th>\n",
       "      <th>Input_Instance_7</th>\n",
       "      <th>Input_Instance_8</th>\n",
       "      <th>Input_Instance_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Input_X_29</th>\n",
       "      <th>Input_X_3</th>\n",
       "      <th>Input_X_4</th>\n",
       "      <th>Input_X_5</th>\n",
       "      <th>Input_X_6</th>\n",
       "      <th>Input_X_7</th>\n",
       "      <th>Input_X_8</th>\n",
       "      <th>Input_X_9</th>\n",
       "      <th>Predicted_Output</th>\n",
       "      <th>Real_Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.164730</td>\n",
       "      <td>0.192093</td>\n",
       "      <td>0.974959</td>\n",
       "      <td>0.899141</td>\n",
       "      <td>0.043330</td>\n",
       "      <td>0.654301</td>\n",
       "      <td>0.457559</td>\n",
       "      <td>0.254529</td>\n",
       "      <td>0.544263</td>\n",
       "      <td>0.988376</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.764892</td>\n",
       "      <td>[0.76596874]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.546057</td>\n",
       "      <td>0.483998</td>\n",
       "      <td>0.773054</td>\n",
       "      <td>0.257339</td>\n",
       "      <td>0.172176</td>\n",
       "      <td>0.269748</td>\n",
       "      <td>0.485393</td>\n",
       "      <td>0.307345</td>\n",
       "      <td>0.165810</td>\n",
       "      <td>0.447696</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.602053</td>\n",
       "      <td>[0.6017551]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.040866</td>\n",
       "      <td>0.305102</td>\n",
       "      <td>0.261795</td>\n",
       "      <td>0.840297</td>\n",
       "      <td>0.502185</td>\n",
       "      <td>0.534389</td>\n",
       "      <td>0.399226</td>\n",
       "      <td>0.666784</td>\n",
       "      <td>0.162492</td>\n",
       "      <td>0.425661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.391674</td>\n",
       "      <td>[0.39263067]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013430</td>\n",
       "      <td>0.925320</td>\n",
       "      <td>0.047815</td>\n",
       "      <td>0.086162</td>\n",
       "      <td>0.205996</td>\n",
       "      <td>0.179840</td>\n",
       "      <td>0.074682</td>\n",
       "      <td>0.171070</td>\n",
       "      <td>0.156180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.547786</td>\n",
       "      <td>[0.5474196]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.755470</td>\n",
       "      <td>0.376943</td>\n",
       "      <td>0.353249</td>\n",
       "      <td>0.602103</td>\n",
       "      <td>0.757552</td>\n",
       "      <td>0.347509</td>\n",
       "      <td>0.867167</td>\n",
       "      <td>0.556594</td>\n",
       "      <td>0.459876</td>\n",
       "      <td>0.543954</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316686</td>\n",
       "      <td>[0.31746706]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87995</th>\n",
       "      <td>0.452325</td>\n",
       "      <td>0.518258</td>\n",
       "      <td>0.026594</td>\n",
       "      <td>0.560850</td>\n",
       "      <td>0.431025</td>\n",
       "      <td>0.900441</td>\n",
       "      <td>0.518797</td>\n",
       "      <td>0.941409</td>\n",
       "      <td>0.689752</td>\n",
       "      <td>0.250133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.546588</td>\n",
       "      <td>[0.5441008]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87996</th>\n",
       "      <td>0.149431</td>\n",
       "      <td>0.605124</td>\n",
       "      <td>0.516706</td>\n",
       "      <td>0.624934</td>\n",
       "      <td>0.244022</td>\n",
       "      <td>0.580957</td>\n",
       "      <td>0.929052</td>\n",
       "      <td>0.603143</td>\n",
       "      <td>0.474768</td>\n",
       "      <td>0.105521</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.520099</td>\n",
       "      <td>[0.52055556]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87997</th>\n",
       "      <td>0.217459</td>\n",
       "      <td>0.040286</td>\n",
       "      <td>0.769093</td>\n",
       "      <td>0.866289</td>\n",
       "      <td>0.111609</td>\n",
       "      <td>0.631405</td>\n",
       "      <td>0.665897</td>\n",
       "      <td>0.131020</td>\n",
       "      <td>0.558484</td>\n",
       "      <td>0.701189</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.335764</td>\n",
       "      <td>[0.34188482]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87998</th>\n",
       "      <td>0.139850</td>\n",
       "      <td>0.773235</td>\n",
       "      <td>0.890499</td>\n",
       "      <td>0.570399</td>\n",
       "      <td>0.633835</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.463572</td>\n",
       "      <td>0.159583</td>\n",
       "      <td>0.718375</td>\n",
       "      <td>0.715948</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333778</td>\n",
       "      <td>[0.33160302]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87999</th>\n",
       "      <td>0.764189</td>\n",
       "      <td>0.299973</td>\n",
       "      <td>0.304070</td>\n",
       "      <td>0.256384</td>\n",
       "      <td>0.876444</td>\n",
       "      <td>0.057283</td>\n",
       "      <td>0.472854</td>\n",
       "      <td>0.450889</td>\n",
       "      <td>0.594486</td>\n",
       "      <td>0.825334</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.526457</td>\n",
       "      <td>[0.5242492]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88000 rows × 214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Input_Instance_0  Input_Instance_1  Input_Instance_2  Input_Instance_3  \\\n",
       "0              0.164730          0.192093          0.974959          0.899141   \n",
       "1              0.546057          0.483998          0.773054          0.257339   \n",
       "2              0.040866          0.305102          0.261795          0.840297   \n",
       "3              0.013430          0.925320          0.047815          0.086162   \n",
       "4              0.755470          0.376943          0.353249          0.602103   \n",
       "...                 ...               ...               ...               ...   \n",
       "87995          0.452325          0.518258          0.026594          0.560850   \n",
       "87996          0.149431          0.605124          0.516706          0.624934   \n",
       "87997          0.217459          0.040286          0.769093          0.866289   \n",
       "87998          0.139850          0.773235          0.890499          0.570399   \n",
       "87999          0.764189          0.299973          0.304070          0.256384   \n",
       "\n",
       "       Input_Instance_4  Input_Instance_5  Input_Instance_6  Input_Instance_7  \\\n",
       "0              0.043330          0.654301          0.457559          0.254529   \n",
       "1              0.172176          0.269748          0.485393          0.307345   \n",
       "2              0.502185          0.534389          0.399226          0.666784   \n",
       "3              0.205996          0.179840          0.074682          0.171070   \n",
       "4              0.757552          0.347509          0.867167          0.556594   \n",
       "...                 ...               ...               ...               ...   \n",
       "87995          0.431025          0.900441          0.518797          0.941409   \n",
       "87996          0.244022          0.580957          0.929052          0.603143   \n",
       "87997          0.111609          0.631405          0.665897          0.131020   \n",
       "87998          0.633835          0.027000          0.463572          0.159583   \n",
       "87999          0.876444          0.057283          0.472854          0.450889   \n",
       "\n",
       "       Input_Instance_8  Input_Instance_9  ...  Input_X_29  Input_X_3  \\\n",
       "0              0.544263          0.988376  ...         1.0        1.0   \n",
       "1              0.165810          0.447696  ...        -0.0        1.0   \n",
       "2              0.162492          0.425661  ...         0.0        1.0   \n",
       "3              0.156180          1.000000  ...         1.0        1.0   \n",
       "4              0.459876          0.543954  ...         1.0        1.0   \n",
       "...                 ...               ...  ...         ...        ...   \n",
       "87995          0.689752          0.250133  ...         0.0        1.0   \n",
       "87996          0.474768          0.105521  ...         1.0        1.0   \n",
       "87997          0.558484          0.701189  ...         1.0        1.0   \n",
       "87998          0.718375          0.715948  ...         1.0        1.0   \n",
       "87999          0.594486          0.825334  ...         1.0        1.0   \n",
       "\n",
       "       Input_X_4  Input_X_5  Input_X_6  Input_X_7  Input_X_8  Input_X_9  \\\n",
       "0           -0.0        1.0        1.0       -0.0        1.0        1.0   \n",
       "1           -0.0        1.0        1.0       -0.0       -0.0        1.0   \n",
       "2            1.0        1.0        1.0        1.0        0.0        1.0   \n",
       "3            1.0        1.0       -0.0        1.0        1.0        1.0   \n",
       "4            1.0        0.0        1.0        1.0        1.0        1.0   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "87995        1.0        1.0        1.0        1.0        1.0        1.0   \n",
       "87996        0.0        1.0        1.0        1.0        1.0       -0.0   \n",
       "87997        1.0        1.0        1.0        0.0        1.0        1.0   \n",
       "87998        1.0       -0.0        1.0       -0.0        1.0        1.0   \n",
       "87999        1.0       -0.0        1.0       -0.0       -0.0        1.0   \n",
       "\n",
       "       Predicted_Output   Real_Output  \n",
       "0              0.764892  [0.76596874]  \n",
       "1              0.602053   [0.6017551]  \n",
       "2              0.391674  [0.39263067]  \n",
       "3              0.547786   [0.5474196]  \n",
       "4              0.316686  [0.31746706]  \n",
       "...                 ...           ...  \n",
       "87995          0.546588   [0.5441008]  \n",
       "87996          0.520099  [0.52055556]  \n",
       "87997          0.335764  [0.34188482]  \n",
       "87998          0.333778  [0.33160302]  \n",
       "87999          0.526457   [0.5242492]  \n",
       "\n",
       "[88000 rows x 214 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88d6732b-0ba5-4cfe-9909-1ebabbb9694b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.concat([train_data_df, test_data_df])\n",
    "file = os.path.join(path_save, 'test_results_30_instance_50_scen.csv')\n",
    "data_df.to_csv(file, index=False)\n",
    "\n",
    "print(\"CSV file saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db980a0a-b709-4bd8-9c7c-70dd30e1222c",
   "metadata": {},
   "source": [
    "### Instantiate the pytorch model for a stored .pth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2db100e7-a9a3-4d86-806a-f6c6ba918cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to hook to the forward pass of all layers\n",
    "def activation_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        activated_neurons = torch.relu(output) > 0  # Using ReLU activation function\n",
    "        activations_dict[name] = activated_neurons\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed682643-9f0f-4a98-b249-c0f2a45d3ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "row = 100\n",
    "    # Create an instance of your model\n",
    "model = MyModel()\n",
    "model.load_state_dict(torch.load(os.path.join(path_save, \"model_weights_30_instance_50_scen.pth\")))\n",
    "\n",
    "# Dictionary to store activated neurons for each layer\n",
    "activations_dict = {}\n",
    "\n",
    "# Register the hook to all layers of the model\n",
    "for name, module in model.named_children():\n",
    "    module.register_forward_hook(activation_hook(name))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(train_dataset.tensors[0][row].reshape(1,-1), train_dataset.tensors[1][row].reshape(1,-1), train_dataset.tensors[2][row].reshape(1,-1))\n",
    "result = np.concatenate([activations_dict[key].numpy().reshape(-1) for key in ['embedding_instance', 'embedding_X', 'embedding_uncern', 'fc1', 'fc2', 'fc3']])\n",
    "# # Print the activated neurons for each layer\n",
    "# for layer_name, activated_neurons in activations_dict.items():\n",
    "#     print(f\"Activated neurons for {layer_name}: {activated_neurons}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c98dffc6-dce7-40b2-b31f-f4d701bcc404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "n_row = train_dataset.tensors[0].shape[0]\n",
    "for i in range(0,n_row):\n",
    "    model = MyModel()\n",
    "    model.load_state_dict(torch.load(os.path.join(path_save, \"model_weights_30_instance_50_scen.pth\")))\n",
    "\n",
    "    # Dictionary to store activated neurons for each layer\n",
    "    activations_dict = {}\n",
    "\n",
    "    # Register the hook to all layers of the model\n",
    "    for name, module in model.named_children():\n",
    "        module.register_forward_hook(activation_hook(name))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(train_dataset.tensors[0][i].reshape(1,-1), train_dataset.tensors[1][i].reshape(1,-1), train_dataset.tensors[2][i].reshape(1,-1))\n",
    "    result.append(np.concatenate([activations_dict[key].numpy().reshape(-1) for key in ['embedding_instance', 'embedding_X', 'embedding_uncern', 'fc1', 'fc2', 'fc3','fc4']]))\n",
    "activations = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9de79fd8-f230-4138-ad75-05dd84dd00b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_test = []\n",
    "n_row_t = test_dataset.tensors[0].shape[0]\n",
    "for i in range(0,n_row_t):\n",
    "    model = MyModel()\n",
    "    model.load_state_dict(torch.load(os.path.join(path_save, \"model_weights_30_instance_50_scen.pth\")))\n",
    "\n",
    "    # Dictionary to store activated neurons for each layer\n",
    "    activations_dict = {}\n",
    "\n",
    "    # Register the hook to all layers of the model\n",
    "    for name, module in model.named_children():\n",
    "        module.register_forward_hook(activation_hook(name))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(test_dataset.tensors[0][i].reshape(1,-1), test_dataset.tensors[1][i].reshape(1,-1), test_dataset.tensors[2][i].reshape(1,-1))\n",
    "    result_test.append(np.concatenate([activations_dict[key].numpy().reshape(-1) for key in ['embedding_instance', 'embedding_X', 'embedding_uncern', 'fc1', 'fc2', 'fc3', 'fc4']]))\n",
    "\n",
    "activations_test = pd.DataFrame(result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1aa4ae27-e657-4e0b-9b9c-b850ce5e18be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.180091 Test: 0.179636\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.520295 Test: 0.518818\n",
      "Train: 0.32042 Test: 0.318318\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.140557 Test: 0.137773\n",
      "Train: 0.349523 Test: 0.351909\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.239477 Test: 0.242091\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.009852 Test: 0.010591\n",
      "Train: 0.340352 Test: 0.338591\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.211148 Test: 0.205409\n",
      "Train: 0.144773 Test: 0.145909\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.095375 Test: 0.0935\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.065182 Test: 0.064273\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.14983 Test: 0.150682\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.195841 Test: 0.191636\n",
      "Train: 0.095341 Test: 0.093636\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.323034 Test: 0.332864\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.135182 Test: 0.134273\n",
      "Train: 0.530182 Test: 0.529273\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.144273 Test: 0.147909\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.265409 Test: 0.263364\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.155364 Test: 0.153545\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.314977 Test: 0.315091\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.100295 Test: 0.098818\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.079591 Test: 0.081636\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.014818 Test: 0.015727\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.459864 Test: 0.460545\n",
      "Train: 0.105182 Test: 0.104273\n",
      "Train: 0.150159 Test: 0.149364\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.310807 Test: 0.309045\n",
      "Train: 0.14 Test: 0.14\n",
      "Train: 0.990625 Test: 0.989773\n",
      "Train: 0.435045 Test: 0.434818\n",
      "Train: 0.98733 Test: 0.987045\n",
      "Train: 0.199682 Test: 0.201273\n",
      "Train: 0.411693 Test: 0.4055\n",
      "Train: 0.365693 Test: 0.371318\n",
      "Train: 0.389386 Test: 0.392455\n",
      "Train: 0.193182 Test: 0.195455\n",
      "Train: 0.434489 Test: 0.4325\n",
      "Train: 0.362545 Test: 0.361182\n",
      "Train: 0.566341 Test: 0.559636\n",
      "Train: 0.559102 Test: 0.563591\n",
      "Train: 0.916386 Test: 0.916273\n",
      "Train: 0.18475 Test: 0.1785\n",
      "Train: 0.596591 Test: 0.595636\n",
      "Train: 0.240102 Test: 0.240591\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.263761 Test: 0.257955\n",
      "Train: 0.262886 Test: 0.258455\n",
      "Train: 0.032966 Test: 0.032636\n",
      "Train: 0.066432 Test: 0.065773\n",
      "Train: 0.523852 Test: 0.529091\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.453432 Test: 0.452273\n",
      "Train: 0.576068 Test: 0.570727\n",
      "Train: 0.249375 Test: 0.2505\n",
      "Train: 0.112352 Test: 0.107591\n",
      "Train: 0.618352 Test: 0.617091\n",
      "Train: 0.000455 Test: 0.000636\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.194875 Test: 0.199591\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.27267 Test: 0.272409\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.08092 Test: 0.081273\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.086159 Test: 0.083409\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.0135 Test: 0.012136\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.37833 Test: 0.376\n",
      "Train: 0.788625 Test: 0.786818\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.375773 Test: 0.3765\n",
      "Train: 0.034398 Test: 0.034\n",
      "Train: 0.16158 Test: 0.162818\n",
      "Train: 0.61575 Test: 0.616773\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.879011 Test: 0.875773\n",
      "Train: 0.712966 Test: 0.708955\n",
      "Train: 0.80058 Test: 0.799818\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.417886 Test: 0.414773\n",
      "Train: 0.10442 Test: 0.105682\n",
      "Train: 0.496307 Test: 0.496227\n",
      "Train: 0.199739 Test: 0.197909\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.123205 Test: 0.123136\n",
      "Train: 0.119489 Test: 0.117818\n",
      "Train: 0.130568 Test: 0.131364\n",
      "Train: 0.103659 Test: 0.105636\n",
      "Train: 0.605159 Test: 0.603091\n",
      "Train: 0.062034 Test: 0.063409\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.03042 Test: 0.03\n",
      "Train: 0.293511 Test: 0.296955\n",
      "Train: 0.1135 Test: 0.113227\n",
      "Train: 0.42242 Test: 0.425909\n",
      "Train: 0.423784 Test: 0.425864\n",
      "Train: 0.261705 Test: 0.263045\n",
      "Train: 0.255091 Test: 0.255818\n",
      "Train: 0.402148 Test: 0.397773\n",
      "Train: 0.147398 Test: 0.15\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.51608 Test: 0.516727\n",
      "Train: 0.260591 Test: 0.260045\n",
      "Train: 0.156295 Test: 0.154727\n",
      "Train: 0.069136 Test: 0.069182\n",
      "Train: 0.066125 Test: 0.064909\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.554432 Test: 0.553682\n",
      "Train: 0.577943 Test: 0.579727\n",
      "Train: 0.116239 Test: 0.117182\n",
      "Train: 0.294557 Test: 0.296955\n",
      "Train: 0.17542 Test: 0.178\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.231818 Test: 0.233364\n",
      "Train: 0.24267 Test: 0.237818\n",
      "Train: 0.156636 Test: 0.154045\n",
      "Train: 0.031943 Test: 0.032273\n",
      "Train: 0.504943 Test: 0.501591\n",
      "Train: 0.09533 Test: 0.096636\n",
      "Train: 0.141011 Test: 0.138364\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.066557 Test: 0.066364\n",
      "Train: 0.5735 Test: 0.575955\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.013136 Test: 0.013045\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.010239 Test: 0.010591\n",
      "Train: 0.24133 Test: 0.242227\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.161159 Test: 0.161636\n",
      "Train: 0.461216 Test: 0.456364\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.200852 Test: 0.2025\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.487227 Test: 0.485682\n",
      "Train: 0.401489 Test: 0.396364\n",
      "Train: 0.106284 Test: 0.103409\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.083273 Test: 0.085682\n",
      "Train: 0.168989 Test: 0.167545\n",
      "Train: 0.026898 Test: 0.026455\n",
      "Train: 0.358398 Test: 0.359773\n",
      "Train: 0.197852 Test: 0.198591\n",
      "Train: 0.671182 Test: 0.673273\n",
      "Train: 0.057 Test: 0.055409\n",
      "Train: 0.179216 Test: 0.174909\n",
      "Train: 0.199727 Test: 0.201818\n",
      "Train: 0.035568 Test: 0.036409\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.383125 Test: 0.379045\n",
      "Train: 0.347341 Test: 0.356136\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.573455 Test: 0.574182\n",
      "Train: 0.184636 Test: 0.182591\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.27983 Test: 0.280364\n",
      "Train: 0.1415 Test: 0.143818\n",
      "Train: 0.139875 Test: 0.138773\n",
      "Train: 0.042398 Test: 0.043227\n",
      "Train: 0.294011 Test: 0.292318\n",
      "Train: 0.053682 Test: 0.054864\n",
      "Train: 0.298466 Test: 0.296273\n",
      "Train: 0.575068 Test: 0.573545\n",
      "Train: 0.518659 Test: 0.521409\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.630852 Test: 0.631136\n",
      "Train: 0.119943 Test: 0.118727\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.114841 Test: 0.1155\n",
      "Train: 0.11017 Test: 0.107955\n",
      "Train: 0.24342 Test: 0.242682\n",
      "Train: 0.116466 Test: 0.119318\n",
      "Train: 0.037045 Test: 0.037364\n",
      "Train: 0.227375 Test: 0.227955\n",
      "Train: 0.10667 Test: 0.106727\n",
      "Train: 0.080875 Test: 0.079182\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.040318 Test: 0.040818\n",
      "Train: 0.067045 Test: 0.067545\n",
      "Train: 0.039636 Test: 0.039545\n",
      "Train: 0.063227 Test: 0.065455\n",
      "Train: 0.918091 Test: 0.918136\n",
      "Train: 0.308398 Test: 0.309636\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.004943 Test: 0.004455\n",
      "Train: 0.089352 Test: 0.089682\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.076045 Test: 0.075773\n",
      "Train: 0.284784 Test: 0.284909\n",
      "Train: 0.01908 Test: 0.019545\n",
      "Train: 0.075443 Test: 0.075182\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.051625 Test: 0.054227\n",
      "Train: 0.121864 Test: 0.1195\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.007341 Test: 0.007045\n",
      "Train: 0.082909 Test: 0.084\n",
      "Train: 0.050409 Test: 0.048864\n",
      "Train: 0.191273 Test: 0.191773\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.143727 Test: 0.144773\n",
      "Train: 0.005398 Test: 0.004818\n",
      "Train: 0.019705 Test: 0.0185\n",
      "Train: 0.218443 Test: 0.217727\n",
      "Train: 0.029489 Test: 0.0295\n",
      "Train: 0.11383 Test: 0.112682\n",
      "Train: 0.077602 Test: 0.077636\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.09142 Test: 0.090227\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.054932 Test: 0.053682\n",
      "Train: 0.035716 Test: 0.036455\n",
      "Train: 0.008375 Test: 0.008455\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.091284 Test: 0.091727\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.064568 Test: 0.066545\n",
      "Train: 0.390977 Test: 0.396091\n",
      "Train: 0.160841 Test: 0.156773\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m Test: 4.5e-05\n",
      "Train: 0.542239 Test: 0.541909\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.010716 Test: 0.011227\n",
      "Train: 0.094875 Test: 0.095\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.136534 Test: 0.134818\n",
      "Train: 0.050955 Test: 0.049909\n",
      "Train: 0.071875 Test: 0.073091\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.019375 Test: 0.018636\n",
      "Train: 1.1e-05 \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.105023 Test: 0.104455\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.45625 Test: 0.455545\n",
      "Train: 0.002125 Test: 0.002636\n",
      "Train: 0.01 Test: 0.010364\n",
      "Train: 0.04142 Test: 0.043409\n",
      "Train: 0.23058 Test: 0.231455\n",
      "Train: 0.000239 Test: 9.1e-05\n",
      "Train: 0.657818 Test: 0.658591\n",
      "Train: 0.004989 Test: 0.005727\n",
      "Train: 0.005114 Test: 0.005455\n",
      "Train: 0.000818 Test: 0.001182\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.000591 Test: 0.000455\n",
      "Train: 0.010375 Test: 0.0105\n",
      "Train: 0.003 Test: 0.003136\n",
      "Train: 0.003614 Test: 0.003364\n",
      "Train: 0.001375 Test: 0.001136\n",
      "Train: 0.551341 Test: 0.551\n",
      "Train: 0.000398 Test: 0.000318\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.000455 Test: 0.000182\n",
      "Train: 0.480114 Test: 0.480364\n",
      "Train: 0.002318 Test: 0.002727\n",
      "Train: 0.00833 Test: 0.007091\n",
      "Train: 0.002534 Test: 0.002955\n",
      "Train: 0.000284 Test: 0.000318\n",
      "Train: 0.966477 Test: 0.9665\n",
      "Train: 0.857125 Test: 0.856545\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 1.1e-05 \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 0.999898 Test: 0.999955\n",
      "\u001b[91mTrain: 0.0\u001b[0m Test: 4.5e-05\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "Train: 1.1e-05 \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n",
      "\u001b[91mTrain: 0.0\u001b[0m \u001b[91mTest: 0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "act_train = (activations.sum()/train_dataset.tensors[0].shape[0]).to_list()\n",
    "act_test = (activations_test.sum()/test_dataset.tensors[0].shape[0]).to_list()\n",
    "\n",
    "for i in range(len(act_train)):\n",
    "    train_activation = act_train[i]\n",
    "    test_activation = act_test[i]\n",
    "\n",
    "    train_text = f\"Train: {round(train_activation, 6)}\"\n",
    "    test_text = f\"Test: {round(test_activation, 6)}\"\n",
    "\n",
    "    # Check if activations are 0 and change color accordingly\n",
    "    if train_activation == 0:\n",
    "        train_text = f\"\\033[91m{train_text}\\033[0m\"  # Red color\n",
    "    if test_activation == 0:\n",
    "        test_text = f\"\\033[91m{test_text}\\033[0m\"  # Red color\n",
    "\n",
    "    print(train_text, test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588443d8-bb2e-4780-b756-cfc0605fca8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb839d6-5fc4-42b9-bdcc-1e59da51d684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
